{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Flight Clustering\n",
    "\n",
    "One of the problems is that all the test flights are in seperate data groupings so each flight test needs to be brought in and cleaned seperately. However, this is costly and requires comparitavely large amounts of memory and time when compared with model training. Both test flight conglomeration as well as test flight training a sequencies of flight tests require some sort of measurement between datasets to determine optimal training scheduling. It is somewhat unclear on how to do this as there is no natural metric so some various ways are presented.\n",
    "\n",
    "## Methods\n",
    "\n",
    "Broadly speaking, there are two types of methods being investigated in this notebook:\n",
    "\n",
    "1. Covariance/Entropy methods\n",
    "2. Loss Function methods\n",
    "\n",
    "The covariance and entropy methods are fairly classical methods from Information Theory. Generating a covariance matrix $\\Sigma$ is fairly easy for uniform discrete time-series and can be used to estimiate maximal bounds on multivariate entropy which can then be used to calculate amounts information that exists in input and output spaces of the flight tests as well as mutual information between flight tests. There are downsides to covariance and entropy calculations and that is the size of memory required to store that much information. This leads to a proposed type of methods called Loss Function methods. Generally speaking, Loss Function methods use trained neural networks and their evaluated loss function over datasets to determine distances between datasets. This is based on the idea that neural networks can be thought of compressed representations of set-set functions or that the neural networks are learning some [kernel representation](https://arxiv.org/abs/2012.00152) as summarly explained [here](https://www.youtube.com/watch?v=ahRPdiCop3E). Manipulating the loss function outputs, it may be possible to have some sort of distance notion between datasets and the ability to learn desired functional outputs.\n",
    "\n",
    "\n",
    "## Input/Output Standardization\n",
    "\n",
    "The test flight datasets are all coming from the Blackbird Dataset and have been standardized of an optimal controller with input and output sensors downsampled to 10 Hz for periods of 1 second. Hence for the $i$th test flight dataset $\\mathcal{D}_i = (\\mathcal{X}, \\mathcal{Y})$ has $N$ samples of the input set $\\mathcal{X} = \\lbrace \\mathbf{X}_j| \\mathbf{X}_j \\in \\mathbb{R}^{240},\\ j=1,\\cdots,N \\rbrace$ and the corresponding output set $\\mathcal{Y} = \\lbrace \\mathbf{Y}_j| \\mathbf{Y}_j \\in \\mathbb{R}^{40},\\ j=1,\\cdots,N \\rbrace$.\n",
    "\n",
    "\n",
    "## Modules\n",
    "\n",
    "Load in the desired python libraries and set up the working directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig  # Graph for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import networkx as nx  # Alternative graph visualizations\n",
    "import numpy as np  # for numerical manipulations\n",
    "import os  # manipulate path\n",
    "import sys\n",
    "import tensorflow as tf  # Manipulate tensorflow neural networks\n",
    "from thesis.data import blackbird_dataset as rbd  # Flight test list\n",
    "from thesis.models import train_optimal_controller as toc  # Generating default neural networks used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance Methods\n",
    "\n",
    "The covariance methods rely on the covariance matrices $\\Sigma$ which are symmetric, positive-definite matrices formed by $\\Sigma_{\\mathbf{X}\\mathbf{X}} = \\mathbb{E}\\left[(\\mathbf{X} - \\mathbb{E}[\\mathbf{X}]) (\\mathbf{X} - \\mathbb{E}[\\mathbf{X}])^T \\right]$ with $\\mathbf{X} \\in \\mathbb{R}^{n}$ and a column random variable vector. With time series, generally the history vector $\\mathbf{X}_h \\in \\mathbb{R}^{n\\times r}$ of $\\mathbf{X}$ is $n$ samplings of a $r$ dimensional space so the covariance matrix can be estimated by $\\Sigma_{\\mathbf{X}\\mathbf{X}} = \\mathbf{X}_h^T \\mathbf{X}_h /(n - 1) - n \\mathbb{E}[\\mathbf{X}]\\mathbb{E}[\\mathbf{X}^T]/(n-1)$. Expanding covariance to the cross-covariance of two random vectors, both in $\\mathbb{R}^r$, with the same $n$ samples as $\\Sigma_{\\mathbf{X}\\mathbf{Y}} = \\mathbf{X}_h^T \\mathbf{Y}_h /(n - 1) - n \\mathbb{E}[\\mathbf{X}]\\mathbb{E}[\\mathbf{Y}^T]/(n-1)$. Using the covariance matrices, the differential entropy as maximized by the multivariate gaussian distribution is $h(\\mathbf{X}) = \\frac{1}{2}\\ln(2\\pi e \\det(\\Sigma_{\\mathbf{X}\\mathbf{X}})) $ which can be used as a measure information stored in the dataset. With information being a notion, the mutual information can also be defined as $I(\\mathbf{X};\\mathbf{Y}) = h(\\mathbf{Y}) - h(\\mathbf{Y}|\\mathbf{X}) = h(\\mathbf{X}) - h(\\mathbf{X}|\\mathbf{Y})$ where $h(\\mathbf{Y}|\\mathbf{X})$ is based on the posterior covariance matrix $\\Sigma_{\\mathbf{Y}|\\mathbf{X}} = \\Sigma_{\\mathbf{YY}} - \\Sigma_{\\mathbf{XY}}^T \\Sigma_{\\mathbf{XX}}^{-1}\\Sigma_{\\mathbf{XY}}$.\n",
    "\n",
    "One of the fundamental issues with doing covariance between datasets is that $\\mathcal{D}_i$ and $\\mathcal{D}_j$ have different number of samples. One of the ways is to potentially rescale the inputs and outputs and approximate but since the optimal controller is based off of time windows, the two datasets cannot be rescales. Instead, both inputs and outputs have an associated time vector so let $t_k' = t_k - \\bar{t}_k$ be the time vector centered around the middle of the maneuver and, without loss of generality, let $|\\mathcal{D}_i| = N < M = |\\mathcal{D}_j|$. Construct a downsample dataset $\\mathcal{D}'_j = \\lbrace (\\mathbf{X}_j, \\mathbf{Y}_j) | (\\mathbf{X}_j, \\mathbf{Y}_j)\\in\\mathcal{D}_j,\\ \\min_{t_j} |t'_i - t'_j|\\ \\forall i = 1,\\cdots,N \\rbrace$ and then constrct the cross-covariance matrices $\\Sigma_{\\mathbf{X}_i \\mathbf{X}'_j}$ and $\\Sigma_{\\mathbf{Y}_i \\mathbf{Y}'_j}$ and this can be used to find cross-entropy and mutual information between datasets $\\mathcal{D}_i$ and $\\mathcal{D}_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def h(Sigma):\n",
    "    # Occasionally det(Sigma_{X|Y}) < 0 so just switch the value\n",
    "    return 0.5*np.log(2.*np.pi*np.exp(1)*np.abs(np.linalg.det(Sigma)))\n",
    "\n",
    "def post_covar(Sxx, Sxy, Syy):\n",
    "    return Sxx - (Sxy @ np.linalg.inv(Syy) @ Sxy)\n",
    "\n",
    "def mutual_information(Sxx, Sxy, Syy):\n",
    "    return h(Sxx) - h(post_covar(Sxx, Sxy, Syy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ampersand', 'Forward', 1.0)\n",
      "('bentDice', 'Forward', 0.5)\n",
      "('bentDice', 'Forward', 1.0)\n",
      "('bentDice', 'Forward', 2.0)\n",
      "('clover', 'Forward', 0.5)\n",
      "('clover', 'Forward', 1.0)\n",
      "('clover', 'Forward', 2.0)\n",
      "('clover', 'Forward', 3.0)\n",
      "('clover', 'Forward', 4.0)\n"
     ]
    }
   ],
   "source": [
    "test_index = [0, 2, 3, 4, 6, 7, 8, 9, 10]  # First 10 valid flight tests for constructing graph\n",
    "\n",
    "# Printing the flight tests\n",
    "for ti in test_index:\n",
    "    print(rbd.TEST_FLIGHT_LIST[ti])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the graph\n",
    "color_dict = {\n",
    "    \"ampersand\": \"red\",\n",
    "    \"bentDice\": \"blue\",\n",
    "    \"clover\": \"green\"\n",
    "}\n",
    "\n",
    "g = ig.Graph.Full(n=len(test_index))\n",
    "g.es[\"weight\"] = 1.\n",
    "g.vs[\"Maneuver\"] = [rbd.TEST_FLIGHT_LIST[ti][0] for ti in test_index]\n",
    "g.vs[\"Speed\"] = [rbd.TEST_FLIGHT_LIST[ti][2] for ti in test_index]\n",
    "g.vs[\"color\"] = [color_dict[maneuver] for maneuver in g.vs[\"Maneuver\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_cov_filename = (\n",
    "    r'../report/distances/covariance/' +\n",
    "    'input_covariance_%i-%i.npy'\n",
    ")\n",
    "\n",
    "# Calculate edge weights based on mutual information\n",
    "for e in g.es:\n",
    "    tf1, tf2 = e.source, e.target\n",
    "    tf1, tf2 = test_index[tf1], test_index[tf2]\n",
    "    Sxx = np.load(input_cov_filename % (tf1, tf1))\n",
    "    Sxy = np.load(input_cov_filename % (tf1, tf2))\n",
    "    Syy = np.load(input_cov_filename % (tf2, tf2))\n",
    "    try:\n",
    "        e[\"weight\"] = mutual_information(Sxx, Sxy, Syy)\n",
    "    except:\n",
    "        print(\"Problem with %i-%i\" % (tf1, tf2))\n",
    "        print(Sxx.shape, Sxy.shape, Syy.shape)\n",
    "        e[\"weight\"] = 0.\n",
    "        \n",
    "    if np.isnan(e[\"weight\"]):\n",
    "        print(\"Problem with %i-%i\" % (tf1, tf2))\n",
    "        if np.any(np.isnan(Sxx)):\n",
    "            print(\"Issue with S_%i-%i\" % (tf1, tf1))\n",
    "        if np.any(np.isnan(Sxy)):\n",
    "            print(\"Issue with S_%i-%i\" % (tf1, tf2))\n",
    "        if np.any(np.isnan(Syy)):\n",
    "            print(\"Issue with S_%i-%i\" % (tf2, tf2))\n",
    "        print(\n",
    "            np.linalg.det(Sxx),\n",
    "            np.linalg.det(Sxy),\n",
    "            np.linalg.det(Syy),\n",
    "            np.linalg.det(post_covar(Sxx, Sxy, Syy))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up plotting characteristics\n",
    "g.vs[\"label\"] = [v[\"Maneuver\"] + \"-\" + str(v[\"Speed\"]) for v in g.vs]\n",
    "\n",
    "# Plotting the resulting graph\n",
    "layout = g.layout_circle()\n",
    "ig.plot(\n",
    "    g, layout=layout, bbox=(500,500),\n",
    "    **{\n",
    "        'edge_width': g.es[\"weight\"],\n",
    "        'margin': 50,\n",
    "        'vertex_label_dist': 2,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_cov_filename = (\n",
    "    r'../report/distances/covariance/' +\n",
    "    'output_covariance_%i-%i.npy'\n",
    ")\n",
    "\n",
    "# Calculate edge weights based on mutual information\n",
    "for e in g.es:\n",
    "    tf1, tf2 = e.source, e.target\n",
    "    tf1, tf2 = test_index[tf1], test_index[tf2]\n",
    "    Sxx = np.load(output_cov_filename % (tf1, tf1))\n",
    "    Sxy = np.load(output_cov_filename % (tf1, tf2))\n",
    "    Syy = np.load(output_cov_filename % (tf2, tf2))\n",
    "    try:\n",
    "        e[\"weight\"] = mutual_information(Sxx, Sxy, Syy)\n",
    "    except:\n",
    "        print(\"Problem with %i-%i\" % (tf1, tf2))\n",
    "        print(Sxx.shape, Sxy.shape, Syy.shape)\n",
    "        e[\"weight\"] = 0.\n",
    "        \n",
    "    if np.isnan(e[\"weight\"]):\n",
    "        print(\"Problem with %i-%i\" % (tf1, tf2))\n",
    "        if np.any(np.isnan(Sxx)):\n",
    "            print(\"Issue with S_%i-%i\" % (tf1, tf1))\n",
    "        if np.any(np.isnan(Sxy)):\n",
    "            print(\"Issue with S_%i-%i\" % (tf1, tf2))\n",
    "        if np.any(np.isnan(Syy)):\n",
    "            print(\"Issue with S_%i-%i\" % (tf2, tf2))\n",
    "        print(\n",
    "            np.linalg.det(Sxx),\n",
    "            np.linalg.det(Sxy),\n",
    "            np.linalg.det(Syy),\n",
    "            np.linalg.det(post_covar(Sxx, Sxy, Syy))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up plotting characteristics\n",
    "g.vs[\"label\"] = [v[\"Maneuver\"] + \"-\" + str(v[\"Speed\"]) for v in g.vs]\n",
    "\n",
    "# Plotting the resulting graph\n",
    "layout = g.layout_circle()\n",
    "ig.plot(\n",
    "    g, \n",
    "    layout=layout, bbox=(500,500),\n",
    "    **{\n",
    "        'edge_width': g.es[\"weight\"],\n",
    "        'margin': 50,\n",
    "        'vertex_label_dist': 2,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up plotting characteristics\n",
    "g.vs[\"label\"] = [v[\"Maneuver\"] + \"-\" + str(v[\"Speed\"]) for v in g.vs]\n",
    "\n",
    "# Plotting the resulting graph\n",
    "layout = g.layout_circle()\n",
    "ig.plot(\n",
    "    g, \n",
    "    \"../report/project-report/figures/pow/mutual-information-graph.png\",\n",
    "    layout=layout, bbox=(500,500),\n",
    "    **{\n",
    "        'edge_width': g.es[\"weight\"],\n",
    "        'margin': 50,\n",
    "        'vertex_label_dist': 2,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function Methods\n",
    "\n",
    "Loss function methods are unlike classical measurements of information relies not the data itself but rather learned representations of the data. Given the dataset $\\mathcal{D}_i$ and having it as a function $\\mathcal{D}_i:\\mathcal{X}_i \\mapsto \\mathcal{Y}_i$, neural networks are trained to find the approximate function $\\hat{f}_i =  \\min_{f} \\mathcal{L}(f(\\mathcal{X}_i), \\mathcal{Y}_i)$ where $\\mathcal{L}$ is a loss function which is a metric of the networks outputs from the labeled true outputs. When talking about multiple datasets, it may be necessary to consider concatination of datasets $\\mathcal{D}_{i,j} = \\mathcal{D}_i \\cup \\mathcal{D}_j$ to find an approximation to some underlying unified function $\\hat{f}_{i,j}$ if it exists. Some properties that you will have is that $\\mathcal{L}(\\hat{f}_i(\\mathcal{X}_i), \\mathcal{Y}_i) \\leq \\mathcal{L}(\\hat{f}_{i,j}(\\mathcal{X}_i), \\mathcal{Y}_i)$ and $\\mathcal{L}(\\hat{f}_j(\\mathcal{X}_j), \\mathcal{Y}_j) \\leq \\mathcal{L}(\\hat{f}_{i,j}(\\mathcal{X}_j), \\mathcal{Y}_j)$ as both $\\hat{f}_i$ and $\\hat{f}_j$ are overfitted to their respectively trained datasets compared to the combined set. However, we also have the following properties $\\mathcal{L}(\\hat{f}_{i,j}(\\mathcal{X}_i), \\mathcal{Y}_i) \\leq \\mathcal{L}(\\hat{f}_j(\\mathcal{X}_i), \\mathcal{Y}_i)$ since $\\hat{f}_{i,j}$ has at least seen $\\mathcal{D}_i$ and has gleaned information and structure while $\\hat{f}_j$ has not. With both upper and lower bounds on the loss function output of the unified function, we can glean that $$ \\mathcal{L}(\\hat{f}_{i}(\\mathcal{X}_i), \\mathcal{Y}_i) + \\mathcal{L}(\\hat{f}_{j}(\\mathcal{X}_j), \\mathcal{Y}_j) \\leq \\mathcal{L}(\\hat{f}_{i,j}(\\mathcal{X}_i), \\mathcal{Y}_i) + \\mathcal{L}(\\hat{f}_{i,j}(\\mathcal{X}_j), \\mathcal{Y}_j) \\leq \\mathcal{L}(\\hat{f}_{j}(\\mathcal{X}_i), \\mathcal{Y}_i) + \\mathcal{L}(\\hat{f}_{i}(\\mathcal{X}_j), \\mathcal{Y}_j)$$ which can be normalized by $\\mathcal{L}(\\hat{f}_{i}(\\mathcal{X}_i), \\mathcal{Y}_i) + \\mathcal{L}(\\hat{f}_{j}(\\mathcal{X}_j), \\mathcal{Y}_j)$ while $\\left(\\dfrac{\\mathcal{L}(\\hat{f}_{j}(\\mathcal{X}_i), \\mathcal{Y}_i) + \\mathcal{L}(\\hat{f}_{i}(\\mathcal{X}_j), \\mathcal{Y}_j)}{\\mathcal{L}(\\hat{f}_{i}(\\mathcal{X}_i), \\mathcal{Y}_i) + \\mathcal{L}(\\hat{f}_{j}(\\mathcal{X}_j), \\mathcal{Y}_j)} - 1 \\right)$ forms a dataset metric. There are a couple immediate applications.\n",
    "\n",
    "1. Given multiple datasets used to train a single neural network, you can determine the optimal strict subset for training that best represents the aggregate dataset.\n",
    "2. Clustering of datasets to form task groups to determine things like number of controllers for autonomous vehicles.\n",
    "3. Verification of dataset quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate adjacency matrix for test flights and load in recorded MSE loss function for the test flights. Invalid is nan\n",
    "N_test_flights = len(rbd.TEST_FLIGHT_LIST)\n",
    "\n",
    "mse_h_file = r\"../report/distances/nn_distance/tf%i_MSE.npy\"\n",
    "raw_adj_matrix = np.zeros((N_test_flights, N_test_flights))\n",
    "for i in range(N_test_flights):\n",
    "    try:\n",
    "        raw_adj_matrix[i] = np.load(mse_h_file % i)\n",
    "    except:\n",
    "        raw_adj_matrix[i,:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate test flights where not able to generate valid test data\n",
    "valid_tf_list = np.any(np.isfinite(raw_adj_matrix), axis=1)\n",
    "valid_tf_indices = np.arange(N_test_flights)[valid_tf_list]\n",
    "valid_tf_list = [rbd.TEST_FLIGHT_LIST[tfi] for tfi in valid_tf_indices]\n",
    "adj_matrix = raw_adj_matrix[valid_tf_indices][:,valid_tf_indices]\n",
    "print(\"Any nan's in adjacency matrix:\", \"Yes\" if np.any(np.isnan(adj_matrix)) else \"No\")\n",
    "assert adj_matrix.shape[0] == adj_matrix.shape[1]\n",
    "print(\"Number of valid test flights: %i\" % len(valid_tf_list))\n",
    "print(valid_tf_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct the metric in adjacency\n",
    "adj_matrix = adj_matrix + adj_matrix.T # Note diagonals are double counted\n",
    "for i in range(adj_matrix.shape[0]):\n",
    "    for j in range(adj_matrix.shape[1]):\n",
    "        if i == j:\n",
    "            continue\n",
    "        else:\n",
    "            # Note: these models are not optimally trained so metric calculated can be < 1\n",
    "            adj_matrix[i,j] = 2.*adj_matrix[i,j]/(adj_matrix[i,i] + adj_matrix[j,j])\n",
    "\n",
    "# Diagonal to zero\n",
    "for i in range(adj_matrix.shape[0]):\n",
    "    adj_matrix[i,i] = 0.\n",
    "\n",
    "for adj in adj_matrix:\n",
    "    print(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iGraph\n",
    "\n",
    "With the graph constructed, we will use the `igraph` python module to display the graph in an attempted interested way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct graph from adjacency matrix\n",
    "adj_list = []\n",
    "for am in adj_matrix:\n",
    "    adj_list.append(list(am))\n",
    "nnd_g = ig.Graph.Adjacency(adj_list, \"UNDIRECTED\")\n",
    "print(\"Is this a weighted graph:\", \"Yes\" if nnd_g.is_weighted() else \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For whatever reason, I need to set the weight.\n",
    "max_adj = np.max(adj_matrix)\n",
    "for e in nnd_g.es:\n",
    "    e[\"weight\"] = 1./adj_matrix[e.source, e.target]  # Simply inverse of the distance\n",
    "    # e[\"weight\"] = max_adj - adj_matrix[e.source, e.target]  # proportional to distance\n",
    "    \n",
    "print(min(nnd_g.es[\"weight\"]), max(nnd_g.es[\"weight\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {\n",
    "    \"ampersand\": \"red\",\n",
    "    \"bentDice\": \"blue\",\n",
    "    \"clover\": \"green\",\n",
    "    \"sid\": \"cyan\",\n",
    "    \"sphinx\": \"purple\",\n",
    "    \"figure8\": \"magenta\",\n",
    "    \"dice\": \"sienna\",\n",
    "    \"tiltedThrice\": \"teal\",\n",
    "    \"3dFigure8\": \"maroon\",\n",
    "    \"picasso\": \"sea green\",\n",
    "    \"thrice\": \"yellow\",\n",
    "    \"oval\": \"tan\",\n",
    "    \"halfMoon\": \"silver\",\n",
    "    \"winter\": \"sky blue\",\n",
    "    \"star\": \"orange\",\n",
    "    \"patrick\": \"dark turquoise\",  # Bug, indigo is a valid X11 colorname but error\n",
    "    \"mouse\": \"goldenrod\"\n",
    "}\n",
    "\n",
    "# Setting up some vertex properties for coloring and plotting\n",
    "nnd_g.vs[\"Maneuver\"] = [tf[0] for tf in valid_tf_list]\n",
    "nnd_g.vs[\"Heading\"] = [tf[1] for tf in valid_tf_list]\n",
    "nnd_g.vs[\"Speed\"] = [tf[2] for tf in valid_tf_list]\n",
    "nnd_g.vs[\"color\"] = [\n",
    "    (color_dict[maneuver] if maneuver in color_dict.keys() else \"black\")\n",
    "    for maneuver in nnd_g.vs[\"Maneuver\"]\n",
    "]\n",
    "# nnd_g.vs[\"label\"] = nnd_g.vs[\"Maneuver\"] # + \"-\" + v[\"Heading\"][0] + \"-\" + str(int(v[\"Speed\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the resulting graph\n",
    "layout = nnd_g.layout_drl()\n",
    "ig.plot(\n",
    "    nnd_g, layout=layout,\n",
    "    bbox=(500,500),\n",
    "    **{\n",
    "        \"edge_width\": np.ceil(np.array(nnd_g.es[\"weight\"]) - 0.985),\n",
    "        # \"edge_width\": np.ceil(nnd_g.es[\"weight\"]/max_adj - 0.999936),  # 4*(nnd_g.es[\"weight\"]/max_adj)**2,\n",
    "        \"margin\": 50,\n",
    "        \"vertex_size\": 5,\n",
    "        \"vertex_label_size\": 20,\n",
    "        \"vertex_label_dist\": 2\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetworkX\n",
    "\n",
    "An alternative to `igraph` module is the `NetworkX` and perhaps it will display the graph in a more visually apparent way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the networkx graph from adj_matrix\n",
    "temp = adj_matrix\n",
    "temp[adj_matrix != 0] = 1./adj_matrix[adj_matrix != 0]\n",
    "G = nx.from_numpy_matrix(temp)  # Should I set diagonals to zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_dict = {\n",
    "    \"ampersand\": \"red\",\n",
    "    \"bentDice\": \"blue\",\n",
    "    \"clover\": \"green\",\n",
    "    \"sid\": \"cyan\",\n",
    "    \"sphinx\": \"purple\",\n",
    "    \"figure8\": \"magenta\",\n",
    "    \"dice\": \"sienna\",\n",
    "    \"tiltedThrice\": \"teal\",\n",
    "    \"3dFigure8\": \"maroon\",\n",
    "    \"picasso\": \"fuchsia\", # Changed\n",
    "    \"thrice\": \"yellow\",\n",
    "    \"oval\": \"tan\",\n",
    "    \"halfMoon\": \"silver\",\n",
    "    \"winter\": \"blue\",\n",
    "    \"star\": \"orange\",\n",
    "    \"patrick\": \"turquoise\",  # Changed\n",
    "    \"mouse\": \"goldenrod\"\n",
    "}\n",
    "\n",
    "edges = G.edges()\n",
    "#weights = [G[u][v] for u,v in edges]\n",
    "node_colors = [color_dict[tf[0]] for tf in valid_tf_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "nx.draw_spring(\n",
    "    G,\n",
    "    node_color=node_colors,\n",
    "    #width=weights\n",
    ")\n",
    "\n",
    "plt.savefig(\"../report/project-report/figures/pow/network-as-cluster.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "nx.draw_kamada_kawai(G)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the flight test\n",
    "\n",
    "After demonstating the multiple ways of attempting to demonstrating illustrating graphs, we use a threshold to attempt to make a distinguishment between a bipartite sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the above graph, generate sets of \"close\" flight tests and \"far\" flight tests for training\n",
    "close_ft = set()\n",
    "far_ft = set()\n",
    "\n",
    "# Iterate over graph edges\n",
    "threshold = 0.95\n",
    "for e in nnd_g.es:\n",
    "    # Check weight threshold\n",
    "    if e[\"weight\"] >= threshold:\n",
    "        # Add valid flight tests to close set\n",
    "        close_ft.add(valid_tf_list[e.source])\n",
    "        close_ft.add(valid_tf_list[e.target])\n",
    "\n",
    "# Iterate over all valid test flights\n",
    "for tf in valid_tf_list:\n",
    "    if tf not in close_ft:\n",
    "        far_ft.add(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print out the results\n",
    "print(\"Number of close flight tests: %i\" % len(close_ft))\n",
    "close_ft_sorted = list(close_ft)\n",
    "close_ft_sorted.sort(key=lambda x: x[0] + x[1] + str(x[2]))\n",
    "for ft in close_ft_sorted:\n",
    "    print(\"\\t%s\" % str(ft))\n",
    "    \n",
    "print(\"\\nNumber of far flight tests: %i\" % len(far_ft))\n",
    "far_ft_sorted = list(far_ft)\n",
    "far_ft_sorted.sort(key=lambda x: x[0] + x[1] + str(x[2]))\n",
    "for ft in far_ft_sorted:\n",
    "    print(\"\\t%s\" % str(ft))\n",
    "    \n",
    "print(\"\\n\\t\\t\\tClose\\tFar\")\n",
    "print(\"Average FT Speed:\\t%0.2f\\t%0.2f\" % (\n",
    "    np.mean(np.array([x[-1] for x in close_ft])),\n",
    "    np.mean(np.array([x[-1] for x in far_ft]))\n",
    "))\n",
    "print(\"Median FT Speed:\\t%0.2f\\t%0.2f\" % (\n",
    "    np.median(np.array([x[-1] for x in close_ft])),\n",
    "    np.median(np.array([x[-1] for x in far_ft]))\n",
    "))\n",
    "print(\"Fraction Constant:\\t%0.2f\\t%0.2f\" % (\n",
    "    len([x for x in close_ft if \"Constant\" in x[1]])/len(close_ft),\n",
    "    len([x for x in far_ft if \"Constant\" in x[1]])/len(far_ft)\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
